"""Option Evaluator Agent implementation."""

import asyncio
import json
from typing import Any

from app.agents.base import Agent
from app.core.exceptions import AgentError, ValidationError
from app.llm.client import get_llm_client
from app.schemas.agents import OptionEvaluatorInput, OptionEvaluatorOutput
from app.schemas.decision import Criterion, OptionScore, OptionScores
from app.schemas.state import PipelineState


def normalize_score(score: float) -> float:
    """
    Normalize score to 0.0-1.0 range.

    Args:
        score: Score value (may be outside 0.0-1.0)

    Returns:
        Normalized score in 0.0-1.0 range
    """
    return max(0.0, min(1.0, score))


def calculate_weighted_total(
    option_scores: list[OptionScore], criteria: list[Criterion]
) -> float:
    """
    Calculate weighted total score for an option.

    Args:
        option_scores: Scores for the option against each criterion
        criteria: List of criteria with weights

    Returns:
        Weighted total score (0.0-1.0)
    """
    # Create a dict for quick lookup
    criterion_weights = {criterion.name: criterion.weight for criterion in criteria}

    total = 0.0
    for option_score in option_scores:
        weight = criterion_weights.get(option_score.criterion_name, 0.0)
        total += option_score.score * weight

    return normalize_score(total)


class OptionEvaluator(Agent[OptionEvaluatorOutput]):
    """
    Option Evaluator Agent: Scores each option against criteria with explicit justifications.

    This is the fourth agent in the pipeline. It evaluates each option against
    the criteria generated by the Criteria Builder, producing scores with
    explicit justifications. Options are scored concurrently for efficiency.
    """

    @property
    def name(self) -> str:
        """Return the agent's name."""
        return "option_evaluator"

    async def _score_single_option(
        self,
        option: str,
        decision_context: str,
        criteria: list[Criterion],
        all_options: list[str],
    ) -> tuple[str, OptionScores]:
        """
        Score a single option against all criteria.

        Args:
            option: The option to score
            decision_context: Decision context
            criteria: List of criteria
            all_options: All options being evaluated (for context)

        Returns:
            Tuple of (option_name, OptionScores)

        Raises:
            AgentError: If scoring fails
        """
        llm_client = get_llm_client()

        # Prepare prompt data
        criteria_for_prompt = [
            {
                "name": criterion.name,
                "weight": criterion.weight,
                "rationale": criterion.rationale,
            }
            for criterion in criteria
        ]

        prompt_data = {
            "decision_context": decision_context,
            "option": option,
            "all_options": json.dumps(all_options),
            "criteria": json.dumps(criteria_for_prompt),
        }

        try:
            # Call LLM with prompt template
            llm_response = await llm_client.complete_with_prompt_template(
                agent_name=self.name,
                template_vars=prompt_data,
            )
        except Exception as e:
            raise AgentError(
                f"LLM call failed for option '{option}': {str(e)}",
                agent_name=self.name,
            ) from e

        # Parse LLM response as JSON
        try:
            # Try to extract JSON from response (handle markdown code blocks)
            response_text = llm_response.strip()
            if response_text.startswith("```json"):
                response_text = response_text[7:]  # Remove ```json
            if response_text.startswith("```"):
                response_text = response_text[3:]  # Remove ```
            if response_text.endswith("```"):
                response_text = response_text[:-3]  # Remove closing ```
            response_text = response_text.strip()

            parsed_response = json.loads(response_text)
        except json.JSONDecodeError as e:
            raise AgentError(
                f"Failed to parse LLM response as JSON for option '{option}': {str(e)}. "
                f"Response: {llm_response[:200]}",
                agent_name=self.name,
            ) from e

        # Build OptionScore objects from parsed response
        option_scores = []
        for score_dict in parsed_response.get("scores", []):
            try:
                # Normalize score to 0.0-1.0 range
                raw_score = score_dict.get("score", 0.0)
                normalized_score = normalize_score(raw_score)

                option_score = OptionScore(
                    criterion_name=score_dict.get("criterion_name", ""),
                    score=normalized_score,
                    justification=score_dict.get("justification", ""),
                )
                option_scores.append(option_score)
            except Exception as e:
                raise ValidationError(
                    f"Failed to build OptionScore: {str(e)}. Score: {score_dict}"
                ) from e

        # Calculate weighted total score
        total_score = calculate_weighted_total(option_scores, criteria)

        # Create OptionScores object
        option_scores_obj = OptionScores(
            total_score=total_score,
            breakdown=option_scores,
        )

        return (option, option_scores_obj)

    async def execute(self, state: PipelineState) -> OptionEvaluatorOutput:
        """
        Execute the Option Evaluator Agent.

        Args:
            state: Pipeline state containing normalized_input and criteria_builder_output

        Returns:
            OptionEvaluatorOutput with scores per option

        Raises:
            AgentError: If agent execution fails
            ValidationError: If output fails schema validation
        """
        # Extract input from PipelineState
        normalized_input = state.normalized_input
        criteria_builder_output = state.criteria_builder_output

        # Get criteria from criteria_builder_output
        if not criteria_builder_output:
            raise AgentError(
                "Criteria Builder output not found in PipelineState",
                agent_name=self.name,
            )

        try:
            # Build criteria list from criteria_builder_output
            criteria_dicts = criteria_builder_output.get("criteria", [])
            criteria_list = [
                Criterion(
                    name=criterion["name"],
                    weight=criterion["weight"],
                    rationale=criterion["rationale"],
                )
                for criterion in criteria_dicts
            ]
        except (KeyError, TypeError) as e:
            raise AgentError(
                f"Failed to extract criteria from criteria_builder_output: {str(e)}",
                agent_name=self.name,
            ) from e

        try:
            # Build OptionEvaluatorInput
            evaluator_input = OptionEvaluatorInput(
                decision_context=normalized_input.get("decision_context", ""),
                options=normalized_input.get("options", []),
                criteria=criteria_list,
            )
        except Exception as e:
            raise AgentError(
                f"Failed to build OptionEvaluatorInput from PipelineState: {str(e)}",
                agent_name=self.name,
            ) from e

        # Score all options concurrently
        scoring_tasks = [
            self._score_single_option(
                option=option,
                decision_context=evaluator_input.decision_context,
                criteria=evaluator_input.criteria,
                all_options=evaluator_input.options,
            )
            for option in evaluator_input.options
        ]

        try:
            # Use asyncio.gather for concurrent scoring
            results = await asyncio.gather(*scoring_tasks)
        except Exception as e:
            raise AgentError(
                f"Failed to score options concurrently: {str(e)}",
                agent_name=self.name,
            ) from e

        # Build scores dictionary
        scores_dict = {option: option_scores for option, option_scores in results}

        # Validate against OptionEvaluatorOutput schema
        try:
            output = OptionEvaluatorOutput(scores=scores_dict)
        except Exception as e:
            raise ValidationError(
                f"OptionEvaluatorOutput validation failed: {str(e)}"
            ) from e

        return output


